{"cells":[{"cell_type":"markdown","metadata":{"id":"4SK_P7x3l9fx"},"source":["# Mount Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsrpG_987WpZ","executionInfo":{"status":"ok","timestamp":1701860017095,"user_tz":-330,"elapsed":118872,"user":{"displayName":"heshan kariyawasam","userId":"17799711020836577437"}},"outputId":"6ed27f52-91e3-4274-8df7-a327f75d3056"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ci5kEqkamBYD"},"source":["# Library installion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzRpTf--55N-"},"outputs":[],"source":["!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Tua44Pd6BXf"},"outputs":[],"source":["!pip install tiktoken"]},{"cell_type":"markdown","metadata":{"id":"WEL4Dr_rmHoj"},"source":["# Library Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOcicBBeRZpY"},"outputs":[],"source":["import openai\n","import os\n","import re\n","import ast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hB3Hy6ON5XPq"},"outputs":[],"source":["import numpy as np\n","import openai\n","import pandas as pd\n","import pickle\n","import tiktoken\n","\n","COMPLETIONS_MODEL = \"text-davinci-003\"\n","EMBEDDING_MODEL = \"dialog_research_text_embedding_002\""]},{"cell_type":"markdown","metadata":{"id":"UxAM4HajmNds"},"source":["# Env Defining"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvWNLEHUFd1K"},"outputs":[],"source":["os.environ[\"OPENAI_API_KEY\"] = \"d60151e62c8e4a4dbabc7239930b6ce5\"\n","openai.api_type = \"azure\"\n","openai.api_base = \"https://genai-research-dialog.openai.azure.com/\"\n","openai.api_version = \"2023-03-15-preview\"\n","openai.api_key = os.getenv(\"OPENAI_API_KEY\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOjGUZkb5XPx"},"outputs":[],"source":["# pip install tiktoken"]},{"cell_type":"markdown","metadata":{"id":"Bm74NM8emVkn"},"source":["# Import Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYg4FEsm5XPz"},"outputs":[],"source":["df_process = pd.read_csv(\"/content/drive/MyDrive/GP/Procuments (1).csv\")\n","df_process = df_process.set_index([\"concat\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OWCu0JS15XP6"},"outputs":[],"source":["document_embeddings_process = pd.read_csv(\"/content/drive/MyDrive/GP/procumentEM (1).csv\")\n","document_embeddings1 = document_embeddings_process.drop(document_embeddings_process.columns[0], axis=1)\n","document_embeddings_T1 = document_embeddings1.transpose()\n","document_embeddings_T1.columns = list(df_process.index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkUmnPmXUNSp"},"outputs":[],"source":["df_general = pd.read_csv(\"/content/drive/MyDrive/GP/notes.csv\")\n","df_general = df_general.set_index([\"content\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xRkMgtSPUNAG"},"outputs":[],"source":["document_embeddings_general = pd.read_csv(\"/content/drive/MyDrive/GP/embeddings1 (1).csv\")\n","document_embeddings2 = document_embeddings_general\n","document_embeddings_T2 = document_embeddings2.transpose()\n","document_embeddings_T2.columns = list(df_general.index)"]},{"cell_type":"markdown","metadata":{"id":"0si-H1S0mzuQ"},"source":["# Embeddings Functions"]},{"cell_type":"markdown","metadata":{"id":"qWueRh4-TxKl"},"source":["for process\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxPsXcC85XP2"},"outputs":[],"source":["def get_embedding(text, model = EMBEDDING_MODEL):\n","  result = openai.Embedding.create(\n","      deployment_id=model,\n","      input=text\n","  )\n","  return result[\"data\"][0][\"embedding\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRzHrQ8C5XP3"},"outputs":[],"source":["def compute_doc_embeddings(df_process):\n","    \"\"\"\n","    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n","\n","    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n","    \"\"\"\n","    return {\n","        idx: get_embedding(r.concat) for idx, r in df_process.iterrows()\n","    }\n","compute_doc_embeddings(df_process)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIGzfVbz5XP4"},"outputs":[],"source":["def load_embeddings(fname):\n","    \"\"\"\n","    Read the document embeddings and their keys from a CSV.\n","\n","    fname is the path to a CSV with exactly these named columns:\n","        \"Heading 1\", \"Heading 2\", \"Heading 3\", \"concat\", \"Content\" ,\"0\", \"1\", ... up to the length of the embedding vectors.\n","    \"\"\"\n","\n","    df_process = pd.read_csv(fname, header=0)\n","    max_dim = max([int(c) for c in df_process.columns if c != \"concat\"])\n","    return {\n","        r.concat1: [r[str(i)] for i in range(max_dim + 1)] for _, r in df_process.iterrows()\n","    }"]},{"cell_type":"markdown","metadata":{"id":"J9nF5HTLT6lA"},"source":["Embeddings for general"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAt1aPFUUAgz"},"outputs":[],"source":["def get_embedding(text, model = EMBEDDING_MODEL):\n","  result = openai.Embedding.create(\n","      deployment_id=model,\n","      input=text\n","  )\n","  return result[\"data\"][0][\"embedding\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hDlXUmDfUAdV"},"outputs":[],"source":["def compute_doc_embeddings(df_general):\n","    \"\"\"\n","    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n","\n","    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n","    \"\"\"\n","    return {\n","        idx: get_embedding(r['content']) for idx, r in df_general.iterrows()\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lE0EOybYUAX5"},"outputs":[],"source":["def load_embeddings(f1name):\n","    \"\"\"\n","    Read the document embeddings and their keys from a CSV.\n","\n","    fname is the path to a CSV with exactly these named columns:\n","        \"context, \"Content\".. up to the length of the embedding vectors.\n","    \"\"\"\n","\n","    df_general = pd.read_csv(f1name, header=0)\n","    max_dim = max([int(c) for c in df_general.columns if c != \"content\"])\n","    return {\n","        r.content1: [r[str(i)] for i in range(max_dim + 1)] for _, r in df_general.iterrows()\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Xs0r1FxddQg"},"outputs":[],"source":["def completions_function(prompt):\n","  response = openai.ChatCompletion.create(\n","        engine=\"dialog_research_gpt35turbo\",\n","        temperature = 0,\n","        messages=[\n","        {\"role\": \"user\", \"content\": prompt}\n","    ]\n","    )\n","  return response['choices'][0]['message']['content']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zrH96vIv98h1"},"outputs":[],"source":["# document_embeddings_T2"]},{"cell_type":"markdown","metadata":{"id":"CETjmCwQnDqV"},"source":["# Similarity Checking Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tsp7_tTp5XQA"},"outputs":[],"source":["def vector_similarity(x, y):\n","    \"\"\"\n","    Returns the similarity between two vectors.\n","\n","    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n","    \"\"\"\n","    return np.dot(np.array(x), np.array(y))\n","\n","def order_document_sections_by_query_similarity(query, context):\n","    \"\"\"\n","    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n","    to find the most relevant sections.\n","\n","    Return the list of document sections, sorted by relevance in descending order.\n","    \"\"\"\n","\n","    query_embedding = get_embedding(query)\n","\n","    document_similarities = sorted([\n","        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in context.items()\n","    ], reverse=True)\n","\n","    return document_similarities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QjemnHmm5XQB"},"outputs":[],"source":["def cosine_similarity(x, y):\n","    \"\"\"\n","    Returns the cosine similarity between two vectors.\n","    \"\"\"\n","    return np.dot(np.array(x), np.array(y)) / (np.linalg.norm(np.array(x)) * np.linalg.norm(np.array(y)))\n","\n","def order_document_sections_by_query_similarity(query, context):\n","    \"\"\"\n","    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n","    to find the most relevant sections.\n","\n","    Return the list of document sections, sorted by relevance in descending order.\n","    \"\"\"\n","    query_embedding = get_embedding(query)\n","\n","    document_similarities = sorted([\n","        (cosine_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in context.items()\n","    ], reverse=True)\n","\n","    return document_similarities\n"]},{"cell_type":"markdown","metadata":{"id":"uM9eoDgFpW-f"},"source":["Genaral"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wAZ3GEx__htm"},"outputs":[],"source":["def cosine_similarity(x, y):\n","    \"\"\"\n","    Returns the cosine similarity between two vectors.\n","    \"\"\"\n","    return np.dot(np.array(x), np.array(y)) / (np.linalg.norm(np.array(x)) * np.linalg.norm(np.array(y)))\n","\n","def order_document_sections_by_query_similarity(query, content):\n","    \"\"\"\n","    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n","    to find the most relevant sections.\n","\n","    Return the list of document sections, sorted by relevance in descending order.\n","    \"\"\"\n","    query_embedding = get_embedding(query)\n","\n","    document_similarities = sorted([\n","        (cosine_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in content.items()\n","    ], reverse=True)\n","\n","    return document_similarities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rxLqm2HoCmUW"},"outputs":[],"source":["def vector_similarity(x, y):\n","    \"\"\"\n","    Returns the similarity between two vectors.\n","\n","    Because OpenAI Embeddings are normalized to length 1, the cosine similarity is the same as the dot product.\n","    \"\"\"\n","    return np.dot(np.array(x), np.array(y))\n","\n","def order_document_sections_by_query_similarity(query, content):\n","    \"\"\"\n","    Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n","    to find the most relevant sections.\n","\n","    Return the list of document sections, sorted by relevance in descending order.\n","    \"\"\"\n","\n","    query_embedding = get_embedding(query)\n","\n","    document_similarities = sorted([\n","        (vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in content.items()\n","    ], reverse=True)\n","\n","    return document_similarities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xADUXmiBpseI"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PCgh0m-MOYvl"},"outputs":[],"source":[" question = \"What is the flow to get budgeted capex LKR 230000?\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcYfmAvXwLc0"},"outputs":[],"source":["def get_category(question):\n","  prompt = \"\"\"\n","  You are given a question from an employee at 'DIALOG AXIATA'. Your task is to determine whether the question falls into the 'process' or 'general' category.\n","\n","  Type 1: Process - The question is related to approval process, recommendation process, steps, flow or processes within the limits of authorities.\n","  Type 2: General - The question is not related to approval process, recommendation process, or processes within the limits of authorities.\n","\n","  Please identify the type of the question.\n","\n","  Example Process Questions:\n","  identify\n","  1. What is the approval process for expenses?\n","  2. How do I request a recommendation letter?\n","\n","\n","  Example General Questions:\n","  1. What is CXO can do?\n","  2. How can I improve my productivity?\n","  3. what is NOTE:10?\n","  4. Who does CXO can reports to?\n","\n","  Question: {question}\n","  Category: {category}\n","  \"\"\"\n","  # Specify the question for classification\n","  user_question = question\n","\n","  # Combine the prompt and user question\n","  combined_prompt = prompt.format(question=user_question, category=\"\")\n","\n","  # Call the OpenAI API to get the category prediction\n","  response = openai.Completion.create(\n","      engine= \"dialog_text_davinci_003\", # You can adjust the model\n","      prompt=combined_prompt,\n","      max_tokens=100  # You can adjust the token limit based on your model's capabilities\n","  )\n","\n","  # Extract the category prediction from the response\n","  category_prediction = response.choices[0].text.strip()\n","\n","  # print(\"Category Prediction:\", category_prediction)  return category_prediction\n","  return category_prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwOYw9tkv1p2"},"outputs":[],"source":["def get_similar_set(question):\n","\n","  category = get_category(question)\n","  print('category in get_similar_set :', category)\n","\n","  if (category == \"Process\"):\n","    document_embeddingsT1 = document_embeddings_T1\n","    ss = order_document_sections_by_query_similarity(question, document_embeddingsT1)[:10]\n","\n","  if (category == \"General\"):\n","    document_embeddingsT2 = document_embeddings_T2\n","    ss = order_document_sections_by_query_similarity(question, document_embeddingsT2)[:4]\n","  return ss, category"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKJJN85COYrF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjxrNuINzyNq"},"outputs":[],"source":["def get_general_question_context(ss):\n","  related_text = ''\n","  for item in range(len(ss)):\n","    related_text = related_text + ss[item][1] + '\\n' + '\\n'\n","  return related_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_F1xaSZOYkW"},"outputs":[],"source":["def give_general_answer(ss,query_context_general,question,model):\n","  prompt = f\"\"\"You will be provided with question and the context delimited by triple backticks.\n","  Your task is to\n","  1. understand the question and context clearly.\n","  2. Use the context to answer the question.\n","  Question:```{question}```\n","\n","  Context= ```{query_context_general}```\n","  \"\"\"\n","  if(model=='chat'):\n","    response = openai.ChatCompletion.create(\n","        engine=\"dialog_research_gpt35turbo\",\n","        temperature = 0,\n","        messages=[\n","        {\"role\": \"user\", \"content\": prompt}\n","    ]\n","    )\n","    return response['choices'][0]['message']['content']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"un6zCBeiz_Ti"},"outputs":[],"source":["loa_context = order_document_sections_by_query_similarity(question, document_embeddings_T1)[:10]\n","query_context = ''\n","index = []\n","for i in range(len(loa_context)):\n","  query_context = query_context  + loa_context[i][1] + '\\n'\n","  index.append(loa_context[i][1])\n","print(query_context)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mINYsv37bmzs"},"outputs":[],"source":["def give_general_answer(ss,query_context_general,question,model):\n","  prompt = f\"\"\"You will be provided with question and the context delimited by triple backticks.\n","  Your task is to\n","  1. understand the question and context clearly.\n","  2. Use the context to answer the question.\n","  Question:```{question}```\n","\n","  Context= ```{query_context_general}```\n","  \"\"\"\n","  if(model=='chat'):\n","    response = openai.ChatCompletion.create(\n","        engine=\"dialog_research_gpt35turbo\",\n","        temperature = 0,\n","        messages=[\n","        {\"role\": \"user\", \"content\": prompt}\n","    ]\n","    )\n","    return response['choices'][0]['message']['content']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxNkq1JGxfFl"},"outputs":[],"source":["def give_best(query_context,question,model):\n","  prompt = f\"\"\"You will be provided with a Question and 10 contexts delimited by triple backticks.\n","\n","  Your task is to follow the steps below.\n","  Step 1. First understand any value mentioned in Question.These numbers may be mostly associated with a currency LKR or USD. \\\n","          Also there can be suffixes associated with a number. For example LKR 2 million or 2 Mn can be valued as 2000000. Identify only the value in the Question.\\\n","          First write value as number.\n","  Step 2. Then identify the lower range and upper range for each context. For a example for a Value between LKR 10000000 and equivalent of LKR 25000000, the lower bound is LKR 10000000 and upper bound is LKR 25000000\n","  Step 3. Then undertand numbers or number ranges in the 10 contexts, from Context 0 to Context 9. These numbers may be mostly associated with a currency LKR or USD. \\\n","          Also there can be suffixes associated with a number. Carefully identify the value of those numbers. For example 2 million or 2 Mn can be valued as 2000000.\n","  Step 4. Then identify the lower range and upper range for each context. For a example for a Value between LKR 10000000 and equivalent of LKR 25000000, the lower bound is LKR 10000000 and upper bound is LKR 25000000\\\n","          Another example is for a Value greater than LKR 45000000, the lower bound is LKR 45000000 and there is no upper bound. If  there is no lower bound , assign lower bound as 0. If there is no upper bound as LKR 5000000000000.\\\n","          output the lower bound and upper bound in a tuple.\n","  Step 5. Give the output of Step 1 and Step 4 as a python Dictionary. Keys : Question Value, Context 1, Context 2 ......\n","\n","\n","  Question:```{question}```\n","\n","  Context 0= ```{query_context[0][1]}```,\n","  Context 1= ```{query_context[1][1]}```,\n","  Context 2= ```{query_context[2][1]}```,\n","  Context 3= ```{query_context[3][1]}```,\n","  Context 4= ```{query_context[4][1]}```,\n","  Context 5= ```{query_context[5][1]}```,\n","  Context 6= ```{query_context[6][1]}```,\n","  Context 7= ```{query_context[7][1]}```,\n","  Context 8= ```{query_context[8][1]}```,\n","  Context 9= ```{query_context[9][1]}```\n","\n","  Return the output of Step 4 Only\n","\n","  \"\"\"\n","\n","  if(model=='chat'):\n","    response = openai.ChatCompletion.create(\n","        engine=\"dialog_research_gpt35turbo\",\n","        temperature = 0,\n","        messages=[\n","        {\"role\": \"user\", \"content\": prompt}\n","    ]\n","    )\n","    return response['choices'][0]['message']['content']\n","\n","\n","  if model == 'davinci':\n","    response = openai.Completion.create(\n","      engine=\"dialog_text_davinci_003\",\n","      #model=\"gpt-35-turbo\",\n","      prompt = prompt,\n","      temperature=0,\n","      max_tokens=2000\n","      )\n","    return response.choices[0].text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnHwgge7yDsk"},"outputs":[],"source":["def give_steps_from_algo(highest_similar_content):\n","  segments = highest_similar_content.split(' ')\n","  results = []\n","  current_segment = ''\n","\n","  for segment in segments:\n","      if segment.startswith(('.')):\n","          if current_segment:\n","              results.append(current_segment.strip())\n","          current_segment = segment\n","      else:\n","          current_segment += ' ' + segment\n","\n","  if current_segment:\n","    results.append(current_segment.strip())\n","\n","  output = '\\n'.join(results)\n","  return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B1B9HBPOwlTh"},"outputs":[],"source":["ss, category = get_similar_set(question)\n","model = 'chat'\n","\n","print('Question:', question)\n","print('Category:', category)\n","\n","if (category == \"Process\"):\n","  number_boundaries = give_best(ss,question,model)\n","  number_boundaries_dict = ast.literal_eval(number_boundaries)\n","  print(number_boundaries_dict)\n","  # highest_similar_content = get_LOA(ss,question,df_process,model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO3CL0ogb32P"},"outputs":[],"source":["matching_indexes = []\n","Question_Set = list()\n","Binary_list = []\n","sentences_list = []\n","\n","for x, y in number_boundaries_dict.items():\n","    if x == 'Question Value':\n","        Question_Value = str(y)\n","    else:\n","        upper_bound = number_boundaries_dict[x][1]\n","        lower_bound = number_boundaries_dict[x][0]\n","        prompt_lower = \"Is value \" + Question_Value + \" lesser than value \" + str(lower_bound) + \" ? Return TRUE or FALSE only\"\n","        prompt_upper = \"Is value \" + Question_Value + \" greater than value \" + str(upper_bound) + \" ? Return TRUE or FALSE only\"\n","        tuple_string = (prompt_lower, prompt_upper)\n","        Question_Set.append(tuple_string)\n","        L = completions_function(prompt_lower)\n","        H = completions_function(prompt_upper)\n","        Binary_list.append((L, H))\n","        if L.__contains__('FALSE') and H.__contains__('FALSE'):\n","            matching_indexes.append(len(Binary_list) - 1)\n","\n","# Extract indexes\n","indexes = matching_indexes\n","\n","for index in matching_indexes:\n","    if 0 <= index < len(Question_Set):\n","        print(ss[index][1])\n","        context_matched = ss[index]\n","        for sentence in context_matched:\n","            sentences_list.append(sentence)\n","\n","# Output indexes and sentences_list\n","print(\"Indexes:\", indexes)\n","print(\"Sentences List:\", sentences_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_fYaPiUcvo_"},"outputs":[],"source":["selected_items = sentences_list[1::2]  # Start at index 1, step by 2\n","print(selected_items)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEs8axKt0tlY"},"outputs":[],"source":["# Strings to remove (modify as needed)\n","strings_to_remove = ['LKR', 'Million', 'Rupees','Mn','million']\n","\n","# Function to remove numbers and specified substrings from a sentence\n","def remove_numbers_and_strings(sentence, strings_to_remove):\n","    # Remove numbers\n","    sentence_without_numbers = re.sub(r'\\d+', '', sentence)\n","\n","    # Remove specified substrings\n","    for string in strings_to_remove:\n","        sentence_without_numbers = sentence_without_numbers.replace(string, '')\n","\n","    return sentence_without_numbers.strip()  # Remove leading/trailing whitespace\n","\n","# # Remove numbers and specified substrings from all sentences in the list\n","cleaned_sentences = [remove_numbers_and_strings(sentence, strings_to_remove) for sentence in selected_items]\n","\n","# Print the cleaned sentences as a list\n","print(cleaned_sentences)\n","\n","\n","sentence_dict = {}\n","\n","for sentence in selected_items:\n","    cleaned_sentence = remove_numbers_and_strings(sentence, strings_to_remove)\n","    sentence_dict[cleaned_sentence] = sentence\n","\n","# Print the dictionary\n","for key, value in sentence_dict.items():\n","    print(f\"Sentence without numbers: {key}\")\n","    print(f\"Original sentence: {value}\")\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAYQ8Skf1isI"},"outputs":[],"source":["import numpy as np\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Tokenize and vectorize the question and phrases\n","vectorizer = TfidfVectorizer()\n","question_vector = vectorizer.fit_transform([question])\n","phrase_vectors = vectorizer.transform(cleaned_sentences)\n","\n","# Calculate cosine similarity between the question and each phrase\n","similarities = cosine_similarity(question_vector, phrase_vectors)\n","\n","# Find the index of the most similar phrase\n","most_similar_index = np.argmax(similarities)\n","most_similar_phrase = cleaned_sentences[most_similar_index]\n","\n","# Print the most similar phrase and its similarity score\n","print(\"Question:\", question)\n","print(\"Most Similar Phrase:\", most_similar_phrase)\n","print(\"Cosine Similarity Score:\", similarities[0, most_similar_index])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s98W_Lq3SLue"},"outputs":[],"source":["sentence_dict[most_similar_phrase]\n","index_ = sentence_dict[most_similar_phrase]\n","print(\"question : \", question)\n","print(\"most similar phrase : \" ,sentence_dict[most_similar_phrase])\n","similar_content = df_process[\"Content\"][index_]\n","similar_content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKlp4qClQj1G"},"outputs":[],"source":["def final_output (similar_content):\n","  print(\"question : \", question)\n","  print(\"most similar phrase : \" ,sentence_dict[most_similar_phrase])\n","  similar_content = df_process[\"Content\"][index_]\n","similar_content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SL2e_9vTOYYo"},"outputs":[],"source":["def get_similar_set(question):\n","\n","  category = get_category(question)\n","  print('category in get_similar_set :', category)\n","\n","  if (category == \"Process\"):\n","    document_embeddingsT1 = document_embeddings_T1\n","    ss = order_document_sections_by_query_similarity(question, document_embeddingsT1)[:10]\n","\n","  if (category == \"General\"):\n","    document_embeddingsT2 = document_embeddings_T2\n","    ss = order_document_sections_by_query_similarity(question, document_embeddingsT2)[:4]\n","  return ss, category"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JwKYlID7IrJ"},"outputs":[],"source":["def get_gen(question,df_general,model):\n","  data = {\"Context 0\" : ss[0][1],\n","          \"Context 1\" : ss[1][1],\n","          \"Context 2\" : ss[2][1],\n","          \"Context 3\" : ss[3][1],}\n","  context = give_general_answer(ss,query_context,question,model)\n","  context = context.strip()\n","  print(context)\n","  contexts = re.findall(r\"(Context [0-3])\", context)\n","  print(contexts)\n","  index = data[contexts[0]]\n","  similar_content = df_general[\"content\"][index]\n","  return similar_content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VvkNu5ZVLPq"},"outputs":[],"source":["def loa_bot(ss,question, df_process, model):\n","  ss, category = get_similar_set(question)\n","  print('Question:', question)\n","  print('Category:', category)\n","\n","  if (category == \"Process\"):\n","    highest_similar_content =df_process[\"Content\"][index_]\n","    steps = give_steps_from_algo(highest_similar_content)\n","    return steps\n","\n","  if (category == \"General\"):\n","    query_context_general = get_general_question_context(ss)\n","    # print(\"context:\",query_context_general)\n","    answer = give_general_answer(ss,query_context_general,question,model)\n","    return answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guYymEbbM1Bt"},"outputs":[],"source":["loa_bot(ss,question, df_process, 'chat')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9fbHQmYc0tW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3SbTkSf6mtI"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jqYxZ55UAjH6"},"source":["Evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLRyQejm5-Kf"},"outputs":[],"source":["def get_completion_from_messages(messages,\n","                                 model=\"gpt-3.5-turbo\",\n","                                 engine=\"dialog_research_gpt35turbo\",\n","                                 temperature=0,\n","                                 max_tokens=1000):\n","    import openai\n","    import os\n","\n","\n","    # Access the environment variables\n","    openai.api_key = \"d60151e62c8e4a4dbabc7239930b6ce5\"\n","    openai.api_type = \"azure\"\n","    openai.api_base = \"https://genai-research-dialog.openai.azure.com/\"\n","    openai.api_version = \"2023-03-15-preview\"\n","\n","\n","\n","    response = openai.ChatCompletion.create(\n","        model=model,\n","        engine=engine,\n","        messages=messages,\n","        temperature=temperature,\n","        max_tokens=max_tokens,\n","    )\n","    return [response.choices[0].message[\"content\"],response.usage]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ss7F8wNa6LRM"},"outputs":[],"source":["def evaluation_prompt(question:str,  ideal_answer:str, bot_answer:str)->str:\n","    print(\"ideal answer, bot answer\",ideal_answer, bot_answer)\n","\n","    system_message = \"\"\"\\\n","    You are an assistant that evaluates how well the customer service agent \\\n","    answers a user question by comparing the response to the ideal (expert) response\n","    Output a single letter and nothing else.\n","    \"\"\"\n","\n","    user_message = f\"\"\"\\\n","You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n","    [BEGIN DATA]\n","    ************\n","    [Question]: {question}\n","    ************\n","    [Expert]: {ideal_answer}\n","    ************\n","    [Submission]: {bot_answer}\n","    ************\n","    [END DATA]\n","\n","Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n","    The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n","    (1) The submitted answer is either a subset, superset or same as the expert answer and is fully consistent with it.\n","    (0) There is a disagreement between the submitted answer and the expert answer.\n","  choice_strings: 0, 1\n","\"\"\"\n","\n","    messages = [\n","        {'role': 'system', 'content': system_message},\n","        {'role': 'user', 'content': user_message}\n","    ]\n","\n","    response = get_completion_from_messages(messages)\n","    return response"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AfO2TfZ6WSH"},"outputs":[],"source":["def loa_bot_evaluation(question, df_process, model):\n","  ss , category = get_similar_set(question)\n","\n","\n","  if (category == \"Process\"):\n","\n","    data = {\"Context 0\" : ss[0][1],\n","          \"Context 1\" : ss[1][1],\n","          \"Context 2\" : ss[2][1],\n","          \"Context 3\" : ss[3][1],\n","          \"Context 4\" : ss[4][1],\n","          \"Context 5\" : ss[5][1],\n","          \"Context 6\" : ss[6][1],\n","          \"Context 7\" : ss[7][1],\n","          \"Context 8\" : ss[8][1],\n","          \"Context 9\" : ss[9][1],}\n","\n","    context = give_best(ss, question,'chat')\n","    context = context.strip()\n","    contexts = re.findall(r\"(Context\\s[0-9])\", context)\n","\n","    index = data[contexts[0]]\n","    print(\"index\",index)\n","    return index\n","\n","  else :\n","    print(\"error\")\n"]},{"cell_type":"markdown","source":["Evaluation\n"],"metadata":{"id":"F-M9lWa_v-Zx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XClcedO6b2n"},"outputs":[],"source":["import json\n","import csv\n","# Placeholder CSV content for demonstration purposes\n","def func_eval_prev(input_file:str, output_file:str ,df_process , model = 'chat'):\n","    with open(input_file, 'r') as csvfile:\n","        reader = csv.reader(csvfile)\n","        # Skip header\n","        next(reader)\n","        csv_content = list(reader)\n","\n","    results = []\n","    for row in csv_content:\n","        user_input = row[0]\n","        ideal_answer = row[1]\n","\n","        response = loa_bot_evaluation(user_input , df_process, model)\n","        output = response[0]\n","        usage = response[1]\n","\n","        evaluation_result = evaluation_prompt(user_input, ideal_answer,response)\n","\n","        results.append([user_input,ideal_answer, response, evaluation_result[0]])\n","\n","    # Write results to a new CSV\n","    with open(output_file, \"w\", newline='') as csvfile:\n","        writer = csv.writer(csvfile)\n","        writer.writerow([\"User Input\",\"Ideal Answer\", \"Bot Answer\", \"Evaluation Result\"])\n","        writer.writerows(results)\n","\n","input_file = \"/content/drive/MyDrive/GP/evaluation.csv\"\n","output_file = \"/content/drive/MyDrive/GP/evaluation_result.csv\"\n","\n","\n","# Process the CSV\n","func_eval_prev(input_file, output_file,df_process,'chat')\n","\n","output_file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtaypptwCKHS"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"11vn6_TbwRcQBX3P5d5UwWqdpJDxfumrA","timestamp":1696921800374},{"file_id":"1mArduORTHWJwH7CrUCQsaTyCX606Tqe9","timestamp":1691424331400},{"file_id":"1x_ftkAQmjIE-n5RZaHJ8hmCI9beHcLma","timestamp":1688019949976},{"file_id":"1eGYYUY4HavoxI1al6JqeBd4TC0eDfkgW","timestamp":1688014581594},{"file_id":"13XEHAcAyTdqn4M9CW3ZJf-vH8mkgBIeJ","timestamp":1686560942880}]},"kernelspec":{"display_name":"yourenvname","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"0725f9013b936eeeaa5c6e1c5b873461ee3d8d8618a888312293dc5b451472ce"}}},"nbformat":4,"nbformat_minor":0}